
        {
            "cells": [
          {
           "cell_type": "code",
           "metadata": {},
            "execution_count": null, "outputs": [], 
           "source": ["#i \"nuget: https://ci.appveyor.com/nuget/diffsharp\"\n",
"#r \"nuget: DiffSharp-lite,0.9.5-local-200721\"\n"]
          }
,
          {
           "cell_type": "markdown",
           "metadata": {},
           
           "source": ["DiffSharp: Differentiable Functional Programming\n",
"================================================\n",
"\n",
"[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/dsyme/DiffSharp/gh-pages?filepath=index.ipynb)\n",
"\n",
"DiffSharp is a tensor library with advanced support for taking derivatives of tensor code using [automatic differentiation](https://en.wikipedia.org/wiki/Automatic_differentiation)\n",
"\n",
"DiffSharp is designed for use in machine learning, probabilistic programming, optimization and other domains.\n",
"\n",
"Using DiffSharp, advanced differentives including gradients, Hessians, Jacobians, directional derivatives, and matrix-free Hessian- and Jacobian-vector\n",
"products are possible. This goes beyond the simple reverse-mode gradients of traditional tensor libraries such as TensorFlow and PyTorch.\n",
"The full expressive capability of the language including control flow while still preserving the ability to take\n",
"advanced differentiation compositions. These can use nested\n",
"forward and reverse AD up to any level, meaning that you can compute exact higher-order derivatives or differentiate functions\n",
"that are internally making use of differentiation. Please see the [API Overview](api-overview.html) page for a list of available operations.\n",
"\n",
"The library is developed by [Atılım Güneş Baydin](https://www.cs.nuim.ie/~gunes/), [Don Syme](https://www.microsoft.com/en-us/research/people/dsyme/)\n",
"and other contributors. Please join us!\n",
"\n",
"\u003e DiffSharp 1.0 is implemented in F# and the default backend uses the PyTorch C++ implementatio of tensors. It is tested on Linux and Windows.\n",
"\n",
"Current Features and Roadmap\n",
"----------------------------\n",
"\n",
"The primary features of DiffSharp 1.0 are:\n",
"\n",
"- _Tensor programming model for F#_\n",
"- _Reference backend for correctness testing_\n",
"- _[PyTorch](https://pytorch.org/) backend for CUDA support and highly optimized native tensor operations_\n",
"- _Nested differentiation for tensors, supporting forward and reverse AD, or any combination thereof, up to any level_\n",
"- _Matrix-free Jacobian- and Hessian-vector products_\n",
"\n",
"See also our [github issues](https://github.com/DiffSharp/DiffSharp/issues/)\n",
"\n",
"Please join with us to help us get the API right and ensure model development with DiffSharp is as succinct and clean as possible/\n",
"\n",
"Quick Usage Example\n",
"-------------------\n"]
          }
,
          {
           "cell_type": "code",
           "metadata": {},
            "execution_count": 2, "outputs": [], 
           "source": ["open DiffSharp\n",
"\n",
"// A scalar-to-scalar function\n",
"let f (x: Tensor) = sin (sqrt x)\n",
"\n",
"// Derivative of f\n",
"let df = dsharp.diff f\n",
"\n",
"// A vector-to-scalar function\n",
"let g (x: Tensor) = exp (x.[0] * x.[1]) + x.[2]\n",
"\n",
"// Gradient of g\n",
"let gg = dsharp.grad g \n",
"\n",
"// Hessian of g\n",
"let hg = dsharp.hessian g\n"]
          }
,
          {
           "cell_type": "markdown",
           "metadata": {},
           
           "source": ["More Info and How to Cite\n",
"-------------------------\n",
"\n",
"If you are using DiffSharp, we would be very happy to hear about it! Please get in touch with us using email or raise any issues you might have [on GitHub](https://github.com/DiffSharp/DiffSharp). We also have a [Gitter chat room](https://gitter.im/DiffSharp/DiffSharp) that we follow.\n",
"\n",
"If you would like to cite this library, please use the following information:\n",
"\n",
"_Atılım Güneş Baydin, Barak A. Pearlmutter, Alexey Andreyevich Radul, Jeffrey Mark Siskind (2015) Automatic differentiation and machine learning: a survey. arXiv preprint. arXiv:1502.05767_ ([link](https://arxiv.org/abs/1502.05767)) ([BibTeX](misc/adml2015.bib))\n",
"\n"]
          }],
            "metadata": {
            "kernelspec": {"display_name": ".NET (F#)", "language": "F#", "name": ".net-fsharp"},
            "langauge_info": {
        "file_extension": ".fs",
        "mimetype": "text/x-fsharp",
        "name": "C#",
        "pygments_lexer": "fsharp",
        "version": "4.5"
        }
        },
            "nbformat": 4,
            "nbformat_minor": 1
        }
        
